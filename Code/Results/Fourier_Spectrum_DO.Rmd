---
title: "Spectrum_for_MCMC"
author: "Montana McLeod"
date: "12/8/2020"
output: pdf_document
---

```{r setup, include=FALSE}
require("knitr")
opts_knit$set(root.dir = "~/Box Sync/McLeod_thesis/Code")
```

```{r}
DOdata <- read.delim("~/Box Sync/McLeod_thesis/Data/DO_Data.txt", comment.char="#")
library(tidyverse)
require(pracma)
require(stats)
require(splines)
require(zoo)
```

# Data Wrangling
```{r echo=FALSE}
# DO less than hypoxic threshold & 150 julian date cut off
data <- as_tibble(DOdata)
Low_hypox <- data %>%
  filter(time >= 150) %>% 
  filter(DO <= 1.43) %>%
  group_by(year) %>%
  summarise(LowDO = n())
#summarise(DO= mean(DO), TotalDO = n())

#total hypoxia values w/ 150 julian date cut off
High_Hypox <- data %>%
  group_by(year) %>%
  filter(time >= 150) %>% 
  summarise(TotalDO = n())

#merged data
hypox <- merge(Low_hypox, High_Hypox, by="year", all=TRUE)

#reorder based on proportion of low DO events
hypox <- hypox %>% 
  mutate(Prop = LowDO/TotalDO)
ord<- order(hypox$Prop, decreasing = T)
hypox <- hypox[ord,]
#hypoxic years: 2020, 2018
#non-hypoxic years: 2015, 2010

#plot good/bad hyopxic years
par(mfrow=c(1,2))
plot(DO~time, data= DOdata[DOdata$year == 2015,], col= "blue", type='l', xlab="Julian Days", ylab = "DO level ml/l", main= "2015")
abline(h=1.43)
plot(DO~time, data= DOdata[DOdata$year == 2018,], col= "red", type='l', xlab="Julian Days", ylab = "DO level ml/l", main='2018')
abline(h=1.43)
```


#  interpolating missing data
```{r}
# aggregated across Julian day and subset by year
# yrs <- seq(2010, 2020, by=1)

missing.DO <- function(data){  
  #start time in late May
  x <- data %>% 
    filter(time >= 150) %>% 
    mutate(Jday = round(time, digits=1))
  
  # factoring in total days per sample period to reformat df
    xx <- x %>% 
      group_by(year) %>% 
      summarise(Tot.days = max(Jday))
   
  # shortening time frame as 10 pts / julian day  
   y <- x %>% 
     group_by(year, Jday) %>%  
    summarise(DO= mean(DO), year= max(year))
  
  # accounts for missing values in dataset 
  y <- spread(y, Jday, DO, fill = NA)
  # establishes set time frame that can later be limited by annual sample period
  z <-pivot_longer(y, cols=c('150':'263.2'), names_to = "Jday", values_to = "DO") #change cols= if change # of jdays****
  z$Jday <- as.numeric(z$Jday)
  z <- inner_join(z, xx, by = 'year')
  
  # removes values that were more than total days in a sample period 
  zz <- z %>% 
    group_by(year, Jday) %>% 
    filter(Jday <= max(Tot.days)) %>% 
    ungroup()
  
  return(zz)
  }
dat_NA <- missing.DO(DOdata)

```

  
# Running Fourier transform to get periodicity 
```{r}

spec.yr <- function(dat, yr){
  dat <- subset(dat, year == yr)
  z = dat
# make X into a timeseries object
  z$Jday <- as.ordered(as.numeric(z$Jday))
  
Dt <- ts(data=z$DO, start = c(min(z$Jday), 1), frequency = 10) #change freq= if change # of jdays****
# note that this assumes sequential evenly-spaced sampling. If that is not the case we have to do some padding...
Dt <- na.fill(Dt, "extend")

# scale to unit variance
Dt_new <- Dt/sd(Dt, na.rm=TRUE)

# detrend
Time = seq(min(dat$Jday), max(dat$Jday), by=0.1) #change by() if change # of jdays****
m <- lm(Dt_new ~ Time)
Dt2 = m$resid
Dt2 = Dt2/sd(Dt2) # rescale to unit variance

# Now do the FFT
f <- seq(from=0, to=0.5, length.out = ceiling(length(Dt2) / 2)) # vector of frequencies
Freq <- abs((fft(Dt2) ) ) / length(Dt2)
Freq = Freq[1:length(f)]


### Red noise significance cutoff (based on Torrence & Compo 1998)
# get ar scale
Dt.ar <- ar(Dt2, na.action=na.pass)
Dt.ar #1.0398
lag1 = Dt.ar[[2]][1]  # Get this value from the ar() operation above #coefficient correlation btw successive pts

# limit the coefficents that are >1 to be just less than 1
lag1 <- ifelse(lag1 >=1, 0.99, lag1)

fft_theor <- (1 - lag1^2) / (1 - 2 * lag1 * cos(f*2*pi) + lag1^2) #eqn 16 w/in cos() should have / length(f) # No - f is already equal to k/N in their notation
fft_theor <- var(Dt2, na.rm=TRUE) * fft_theor / (2 * length(f)) #rescaling for unit variance?
chi2 = qchisq(0.05, 2, lower.tail=FALSE) / 2
signif = chi2*fft_theor #significance level

#create dataframe for Periodicity, frequency and significance 
Periodicity <- data.frame(Year = yr, 
                          Period = 1/f, 
                          Freq_sq = Freq^2,
                          Signf = signif)

  # Simulating new datasets based on the FFT
  N = 100
 Sim_DO_matrix <- matrix(NA, nrow = 100, ncol = length(Dt2))
  
  #simulated ffts all saved in one column
  for( j in 1:N){
    # We do this by randomizing the phase (imaginary part) of the FFT spectrum
    Theta <- runif(n=length(Dt2)) * 2 * pi # 2pi converts to radians
    # Now do inverse FFT with modulus (real part) of original FFT but randomized phase
    Z = Re(fft(Dt2)) * exp(1i*Theta) 
    Z = Re(ifft(Z))
    sim_DO = as.vector(Z * sd(Dt) + mean(Dt))
    ifelse(sim_DO <= 0, 0, sim_DO)
    Sim_DO_matrix[j,] <- sim_DO
    
  }
  # make negative simDO = 0
#Sim_DO_matrix[Sim_DO_matrix <= 0] <- 0

  Sims <- list( Periodicity = Periodicity,
                Sim_DO_matrix = Sim_DO_matrix)
  
  return(Sims)
}
  good_yr <- spec.yr(dat_NA, 2015)
  bad_yr <- spec.yr(dat_NA, 2018)

DsD <- rbind(good_yr$Periodicity, bad_yr$Periodicity)
# divide x by 10 -x axis by days

#removes any infinity values that mess with visualization 
DsD <- do.call(data.frame,lapply(DsD, function(x) replace(x, is.infinite(x),NA)))
DsD <- na.omit(DsD)

#save(dat_NA, good_yr, bad_yr, DsD , file= "~/Box Sync/McLeod_thesis/Data/DO_sims.Rdata")
```


