---
title: "Spectrum_for_MCMC"
author: "Montana McLeod"
date: "12/8/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

setwd("~/Box Sync/McLeod_thesis/code")
DOdata <- read.delim("~/Box Sync/McLeod_thesis/Data/DO_Data.txt", comment.char="#")
library(tidyverse)
library(pracma)
require(stats)
require(splines)
require(zoo)
```

# Data Wrangling
```{r echo=FALSE}
# DO less than hypoxic threshold & 150 julian date cut off
data <- as_tibble(DOdata)
Low_hypox <- data %>%
  filter(time >= 150) %>% 
  filter(DO <= 1.43) %>%
  group_by(year) %>%
  summarise(LowDO = n())
#summarise(DO= mean(DO), TotalDO = n())

#total hypoxia values w/ 150 julian date cut off
High_Hypox <- data %>%
  group_by(year) %>%
  filter(time >= 150) %>% 
  summarise(TotalDO = n())

#merged data
hypox <- merge(Low_hypox, High_Hypox, by="year", all=TRUE)

#reorder based on proportion of low DO events
hypox <- hypox %>% 
  mutate(Prop = LowDO/TotalDO)
ord<- order(hypox$Prop, decreasing = T)
hypox <- hypox[ord,]
#hypoxic years: 2020, 2018
#non-hypoxic years: 2015, 2010

#plot good/bad hyopxic years
par(mfrow=c(1,2))
plot(DO~time, data= DOdata[DOdata$year == 2015,], col= "blue", type='l')
abline(h=1.43)
plot(DO~time, data= DOdata[DOdata$year == 2018,], col= "red", type='l')
abline(h=1.43)
```


# Plots after interpolation
```{r}
# aggregated across Julian day and subset by year
# yrs <- seq(2010, 2020, by=1)

missing.DO <- function(data){  
  #start time in late May
  x <- data %>% 
    filter(time >= 150) %>% 
    mutate(Jday = round(time, digits=0))
  
  # factoring in total days per sample period to reformat df
    xx <- x %>% 
      group_by(year) %>% 
      summarise(Tot.days = max(Jday))
   
  # shortening time frame as 10 pts / julian day  
   y <- x %>% 
     group_by(year, Jday) %>%  
    summarise(DO= mean(DO), year= max(year))
  
  # accounts for missing values in dataset 
  y <- spread(y, Jday, DO, fill = NA)
  # establishes set time frame that can later be limited by annual sample period
  z <-pivot_longer(y, cols=c('150':'263'), names_to = "Jday", values_to = "DO")
  z$Jday <- as.numeric(z$Jday)
  z <- inner_join(z, xx, by = 'year')
  
  # removes values that were more than total days in a sample period 
  zz <- z %>% 
    group_by(year, Jday) %>% 
    filter(Jday <= max(Tot.days)) %>% 
    ungroup()
  
  return(zz)
  }
dat_NA <- missing.DO(DOdata)


#intrp.DO <- function(dat, yr){
 # dat <- dat %>% 
    #filter(year == yrs[i])
  #x <- as.data.frame(dat[is.na(dat$DO),])
  
  #intrp <-loess(DO ~ Jday, dat, span= 0.2, control = loess.control(surface = "direct")) #interpolates
  #Unif_t <- seq(min(dat$Jday), max(dat$Jday), by = 0.1) #creates the sequence of julian days we want

#  y <- predict(intrp, data.frame(Jday = Unif_t), se=F) #predicts the missing values of DO
  
 #   z <- data.frame(Jday = Unif_t, DO =  y)
  #  zz <- inner_join(z, x, by = c('Jday', 'DO'))
  #dat$Year <- rep(yr, nrow(dat))
  # dat <- dat[ ,c(3,1:2)]
  # colnames(dat) <- c("Year", "Jday", "DO")
  #return(dat)
#}
  
  #table(diff(interp_df$Jday)) 
  
  ggplot(dat_NA, aes(Jday, DO, color=year))+
  geom_line()+
  scale_fill_brewer(palette = "Accent")+
  facet_wrap(~year, 4)+
  geom_hline(yintercept = 1.43, color= "red")
  
```


# using the interpolated results
```{r}

spec.yr <- function(dat, yr){
  dat <- subset(dat, year == yr)
  z = dat
# make X into a timeseries object
  z$Jday <- as.ordered(as.numeric(z$Jday))
  
Dt <- ts(data=z$DO, start = c(min(z$Jday), 1), frequency = 1) #set frequency to 1 : everyday
# note that this assumes sequential evenly-spaced sampling. If that is not the case we have to do some padding...
Dt <- na.fill(Dt, "extend")

# scale to unit variance
Dt_new <- Dt/sd(Dt, na.rm=TRUE)

# detrend
Time = seq(min(dat$Jday), max(dat$Jday), by=1)
m <- lm(Dt_new ~ Time)
Dt2 = m$resid
Dt2 = Dt2/sd(Dt2) # rescale to unit variance

# Now do the FFT
f <- seq(from=0, to=0.5, length.out = ceiling(length(Dt2) / 2)) # vector of frequencies
Freq <- abs((fft(Dt2) ) ) / length(Dt2)
Freq = Freq[1:length(f)]


### Red noise significance cutoff (based on Torrence & Compo 1998)
# get ar scale
Dt.ar <- ar(Dt2, na.action=na.pass)
Dt.ar #1.0398
lag1 = Dt.ar[[2]][1]  # Get this value from the ar() operation above #coefficient correlation btw successive pts

# limit the coefficents that are >1 to be just less than 1
lag1 <- ifelse(lag1 >=1, 0.99, lag1)

fft_theor <- (1 - lag1^2) / (1 - 2 * lag1 * cos(f*2*pi) + lag1^2) #eqn 16 w/in cos() should have / length(f) # No - f is already equal to k/N in their notation
fft_theor <- var(Dt2, na.rm=TRUE) * fft_theor / (2 * length(f)) #rescaling for unit variance?
chi2 = qchisq(0.05, 2, lower.tail=FALSE) / 2
signif = chi2*fft_theor #significance level

# make into dataframe for ggplot
DsD <-  data.frame(Period = 1/f, 
                   Freq_sq = Freq^2,
                   Signf = signif) 
DsD <- cbind(rep(yr, nrow(DsD)), DsD)

# Simulating new datasets based on the FFT

N = 100
ZZ <- matrix(NA, nrow = 100, ncol = length(f))
for( j in 1:N){
  # We do this by randomizing the phase (imaginary part) of the FFT spectrum
  Theta <- runif(n=length(f)) * 2 * pi # 2pi converts to radians
  # Now do inverse FFT with modulus (real part) of original FFT but randomized phase
  Z = Re(fft(Dt2[1:length(f)])) * exp(1i*Theta) 
  Z = Re(ifft(Z))
  ZZ[j,] = Z * sd(Dt) + mean(Dt)
}
DsD$Sim_DO <- ZZ[N,]

# make negative simDO - 0
DsD[DsD$Sim_DO <= 0, 5] <- 0
return(DsD)
}

DsD <- rbind(spec.yr(dat_NA, 2015), spec.yr(dat_NA, 2018))
colnames(DsD) <- c("Year", "Period", "Freq_sq", "Signf", "Sim_DO")
save(DsD, file= "DO_sims.Rdata")
# divide x by 10 -x axis by days

#removes any infinity values that mess with visualization 
DsD <- do.call(data.frame,lapply(DsD, function(x) replace(x, is.infinite(x),NA)))
DsD <- na.omit(DsD)
plot(DsD$Sim_DO, type='l')

ggplot(DsD, aes(x=Period, Freq_sq, color=Year))+
  geom_line()+
  facet_wrap(~Year) 
  #scale_x_continuous() #made dataframe by the day instead of 10 timepts per day

ggplot(DsD, aes(Period, Freq_sq))+
  geom_line()+
  geom_line(aes(Period, Signf, color= "red"))+
  facet_wrap(~Year)
# both lag1 auto correlations are >1 and I set limit of >1 = 0.99 so we have a large amount of values over 1
## IS THERE A STEP IM MISSING THATS LEADING TO POOR AUTOCORRELATION COEFFICIENTS?
```



## Added to the Spec.yr() fxn

```{r}
# get ar scale
Dt.ar <- ar(Dt2, na.action=na.pass)
Dt.ar #1.0398

# Red noise significance cutoff (based on Torrence & Compo 1998)
lag1 = Dt.ar[[2]][1]  # Get this value from the ar() operation above #coefficient correlation btw successive pts
fft_theor <- (1 - lag1^2) / (1 - 2 * lag1 * cos(f*2*pi) + lag1^2) #eqn 16 w/in cos() should have / length(f) # No - f is already equal to k/N in their notation
fft_theor <- var(Dt2, na.rm=TRUE) * fft_theor / (2 * length(f)) #rescaling for unit variance?
chi2 = qchisq(0.05, 2, lower.tail=FALSE) / 2
signif = chi2*fft_theor

good_yr$signif = signif

ggplot(DsD, aes(Period, Freq_sq))+
  geom_line()+
  geom_line(aes(Period, Signf, color= "red"))+
  facet_wrap(~Year)

```