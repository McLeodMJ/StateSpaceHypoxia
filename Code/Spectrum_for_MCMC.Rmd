---
title: "Spectrum_for_MCMC"
author: "Montana McLeod"
date: "12/8/2020"
output: pdf_document
---

```{r setup, include=FALSE}
require("knitr")
opts_knit$set(root.dir = "~/Box Sync/McLeod_thesis/code")
```

```{r}
DOdata <- read.delim("~/Box Sync/McLeod_thesis/Data/DO_Data.txt", comment.char="#")
library(tidyverse)
library(pracma)
require(stats)
require(splines)
require(zoo)
```

# Data Wrangling
```{r echo=FALSE}
# DO less than hypoxic threshold & 150 julian date cut off
data <- as_tibble(DOdata)
Low_hypox <- data %>%
  filter(time >= 150) %>% 
  filter(DO <= 1.43) %>%
  group_by(year) %>%
  summarise(LowDO = n())
#summarise(DO= mean(DO), TotalDO = n())

#total hypoxia values w/ 150 julian date cut off
High_Hypox <- data %>%
  group_by(year) %>%
  filter(time >= 150) %>% 
  summarise(TotalDO = n())

#merged data
hypox <- merge(Low_hypox, High_Hypox, by="year", all=TRUE)

#reorder based on proportion of low DO events
hypox <- hypox %>% 
  mutate(Prop = LowDO/TotalDO)
ord<- order(hypox$Prop, decreasing = T)
hypox <- hypox[ord,]
#hypoxic years: 2020, 2018
#non-hypoxic years: 2015, 2010

#plot good/bad hyopxic years
par(mfrow=c(1,2))
plot(DO~time, data= DOdata[DOdata$year == 2015,], col= "blue", type='l')
abline(h=1.43)
plot(DO~time, data= DOdata[DOdata$year == 2018,], col= "red", type='l')
abline(h=1.43)
```


# Plots after interpolation
```{r}
# aggregated across Julian day and subset by year
# yrs <- seq(2010, 2020, by=1)

missing.DO <- function(data){  
  #start time in late May
  x <- data %>% 
    filter(time >= 150) %>% 
    mutate(Jday = round(time, digits=1))
  
  # factoring in total days per sample period to reformat df
    xx <- x %>% 
      group_by(year) %>% 
      summarise(Tot.days = max(Jday))
   
  # shortening time frame as 10 pts / julian day  
   y <- x %>% 
     group_by(year, Jday) %>%  
    summarise(DO= mean(DO), year= max(year))
  
  # accounts for missing values in dataset 
  y <- spread(y, Jday, DO, fill = NA)
  # establishes set time frame that can later be limited by annual sample period
  z <-pivot_longer(y, cols=c('150':'263.2'), names_to = "Jday", values_to = "DO") #change cols= if change # of jdays****
  z$Jday <- as.numeric(z$Jday)
  z <- inner_join(z, xx, by = 'year')
  
  # removes values that were more than total days in a sample period 
  zz <- z %>% 
    group_by(year, Jday) %>% 
    filter(Jday <= max(Tot.days)) %>% 
    ungroup()
  
  return(zz)
  }
dat_NA <- missing.DO(DOdata)

 
  ggplot(dat_NA, aes(Jday, DO, color=year))+
  geom_line()+
  scale_fill_brewer(palette = "Accent")+
  facet_wrap(~year, 4)+
  geom_hline(yintercept = 1.43, color= "red")
  
```

# function for timeseries simulations & FFT
```{r}
spec.yr <- function(dat, yr){
  dat <- subset(dat, year == yr)
  z = dat
  # make X into a timeseries object
  z$Jday <- as.ordered(as.numeric(z$Jday))
  
  Dt <- ts(data=z$DO, start = c(min(z$Jday), 1), frequency = 10) #change freq= if change # of jdays****
  # note that this assumes sequential evenly-spaced sampling. If that is not the case we have to do some padding...
  Dt <- na.fill(Dt, "extend")
  
  # scale to unit variance
  Dt_new <- Dt/sd(Dt, na.rm=TRUE)
  
  # detrend
  Time = seq(min(dat$Jday), max(dat$Jday), by=0.1) #change by() if change # of jdays****
  m <- lm(Dt_new ~ Time)
  Dt2 = m$resid
  Dt2 = Dt2/sd(Dt2) # rescale to unit variance
  
  
  ### Red noise significance cutoff (based on Torrence & Compo 1998)
  # get ar scale
  Dt.ar <- ar(Dt2, na.action=na.pass)
  Dt.ar #1.0398
  lag1 = Dt.ar[[2]][1]  # Get this value from the ar() operation above #coefficient correlation btw successive pts
  
  # limit the coefficents that are >1 to be just less than 1
  lag1 <- ifelse(lag1 >=1, 0.99, lag1)
  
  fft_theor <- (1 - lag1^2) / (1 - 2 * lag1 * cos(Dt2*2*pi) + lag1^2) #eqn 16 w/in cos() should have / length(f) # No - f is already equal to k/N in their notation
  fft_theor <- var(Dt2, na.rm=TRUE) * fft_theor / (2 * length(Dt2)) #rescaling for unit variance?
  chi2 = qchisq(0.05, 2, lower.tail=FALSE) / 2
  signif = chi2*fft_theor #significance level
 
  # Simulating new datasets based on the FFT
  N = 100
  Sim_DO <- data.frame(sim = rep(seq(1:N), each = nrow(dat)) ,
                       DO = rep(NA,N*nrow(dat)))
  Sim_DO_matrix <- matrix(NA, nrow = 100, ncol = length(Dt2))
  
  #simulated ffts all saved in one column
  for( j in 1:N){
    # We do this by randomizing the phase (imaginary part) of the FFT spectrum
    Theta <- runif(n=length(Dt2)) * 2 * pi # 2pi converts to radians
    # Now do inverse FFT with modulus (real part) of original FFT but randomized phase
    Z = Re(fft(Dt2)) * exp(1i*Theta) 
    Z = Re(ifft(Z))
    sim_DO = Z * sd(Dt) + mean(Dt)
    Sim_DO[Sim_DO$sim == j,2] <- sim_DO
    Sim_DO_matrix[j,] <- sim_DO
  }
  # make negative simDO = 0
# Sim_DO[ Sim_DO[,2] < 0] = 0
 
  Sim_DO$signif <- signif
  Sim_DO$dt2 <- Dt2
  Sim_DO$time <- 1: nrow(Sim_DO)
  
  Sims <- list( Sim_DO = Sim_DO,
                Sim_DO_matrix = Sim_DO_matrix)
  
  return(Sims)
}
  good_yr <- spec.yr(dat_NA, 2015)
  bad_yr <- spec.yr(dat_NA, 2018)
  save(good_yr, bad_yr, file= "DO_sims_New.Rdata")
  View(good_yr$Sim_DO_matrix)
```
  
  
  
# FFT based on Simulations and detrend
```{r}
  # Now do the FFT
  f <- seq(from=0, to=0.5, length.out = ceiling(nrow(bad_yr) / 2)) # vector of frequencies
  F0 <- abs((fft(bad_yr$dt2[1:nrow(bad_yr)/N]) ) ) / length(bad_yr$dt2[1:nrow(bad_yr)/N])
  F0 = F0[1:length(f)]
  Freq <- abs((fft(bad_yr$DO) ) ) / length(bad_yr$DO)
  Freq = Freq[1:length(f)]
  fft_sims <- data.frame(Period = 1/f, F0.sq = F0^2, Freq.sq = Freq^2)
  
 fft_sims <- do.call(data.frame,lapply(fft_sims, function(x) replace(x, is.infinite(x),NA)))
  fft_sims <- na.omit(fft_sims)
  
  # simulated data for 100-yrs (these are shown as continuous but really gap in ts for off-season months)
  bad <- ggplot(bad_yr, aes(time, DO))+
           geom_line(alpha=0.7) +
          labs(title= "Low DO")
  good <- ggplot(good_yr, aes(time, DO))+
           geom_line(alpha=0.5, color = "blue")+
            labs(title= "Standard DO")
  good+bad
  
# plot the data
plot(1/f,log(Freq^2),type='l',xlim=c(0,1100), xlab= "Period", ylab= "Freq.sq", main= "Low DO Year")
lines(1/f,log(F0^2),type='l',col='red')
legend("bottom", legend = c("Simulations", "detrend"), pch = c(15, 15), col = c("black", "red"))
```


#OLD - remove if above is cleared #########

# using the interpolated results
```{r}

spec.yr <- function(dat, yr){
  dat <- subset(dat, year == yr)
  z = dat
# make X into a timeseries object
  z$Jday <- as.ordered(as.numeric(z$Jday))
  
Dt <- ts(data=z$DO, start = c(min(z$Jday), 1), frequency = 10) #change freq= if change # of jdays****
# note that this assumes sequential evenly-spaced sampling. If that is not the case we have to do some padding...
Dt <- na.fill(Dt, "extend")

# scale to unit variance
Dt_new <- Dt/sd(Dt, na.rm=TRUE)

# detrend
Time = seq(min(dat$Jday), max(dat$Jday), by=0.1) #change by() if change # of jdays****
m <- lm(Dt_new ~ Time)
Dt2 = m$resid
Dt2 = Dt2/sd(Dt2) # rescale to unit variance

# Now do the FFT
f <- seq(from=0, to=0.5, length.out = ceiling(length(Dt2) / 2)) # vector of frequencies
Freq <- abs((fft(Dt2) ) ) / length(Dt2)
Freq = Freq[1:length(f)]


### Red noise significance cutoff (based on Torrence & Compo 1998)
# get ar scale
Dt.ar <- ar(Dt2, na.action=na.pass)
Dt.ar #1.0398
lag1 = Dt.ar[[2]][1]  # Get this value from the ar() operation above #coefficient correlation btw successive pts

# limit the coefficents that are >1 to be just less than 1
lag1 <- ifelse(lag1 >=1, 0.99, lag1)

fft_theor <- (1 - lag1^2) / (1 - 2 * lag1 * cos(f*2*pi) + lag1^2) #eqn 16 w/in cos() should have / length(f) # No - f is already equal to k/N in their notation
fft_theor <- var(Dt2, na.rm=TRUE) * fft_theor / (2 * length(f)) #rescaling for unit variance?
chi2 = qchisq(0.05, 2, lower.tail=FALSE) / 2
signif = chi2*fft_theor #significance level

# make into dataframe for ggplot
DsD <-  data.frame(Period = 1/f, 
                   Freq_sq = Freq^2,
                   Signf = signif) 
DsD <- cbind(rep(yr, nrow(DsD)), DsD)

# Simulating new datasets based on the FFT
N = 100
ZZ <- matrix(NA, nrow = 100, ncol = length(f))
for( j in 1:N){
  # We do this by randomizing the phase (imaginary part) of the FFT spectrum
  Theta <- runif(n=length(f)) * 2 * pi # 2pi converts to radians
  # Now do inverse FFT with modulus (real part) of original FFT but randomized phase
  Z = Re(fft(Dt2[1:length(f)])) * exp(1i*Theta) 
  Z = Re(ifft(Z))
  ZZ[j,] = Z * sd(Dt) + mean(Dt)
}

#this takes only one simulation when I believe we want multiple
DsD$Sim_DO <- ZZ[N,]

# make negative simDO - 0
DsD[DsD$Sim_DO <= 0, 5] <- 0
return(DsD)
}

DsD <- rbind(spec.yr(dat_NA, 2015), spec.yr(dat_NA, 2018))
colnames(DsD) <- c("Year", "Period", "Freq_sq", "Signf", "Sim_DO")
save(DsD, file= "DO_sims.Rdata")
# divide x by 10 -x axis by days

#removes any infinity values that mess with visualization 
DsD <- do.call(data.frame,lapply(DsD, function(x) replace(x, is.infinite(x),NA)))
DsD <- na.omit(DsD)
plot(DsD$Sim_DO, type='l')


ggplot(DsD, aes(Period, Freq_sq))+
  geom_line()+
  geom_line(aes(Period, Signf, color= "red"))+
  facet_wrap(~Year)
# both lag1 auto correlations are >1 and I set limit of >1 = 0.99 so we have a large amount of values over 1

## IS THERE A STEP IM MISSING THATS LEADING TO POOR AUTOCORRELATION COEFFICIENTS?
```



## Added to the Spec.yr() fxn

```{r}
# get ar scale
Dt.ar <- ar(Dt2, na.action=na.pass)
Dt.ar #1.0398

# Red noise significance cutoff (based on Torrence & Compo 1998)
lag1 = Dt.ar[[2]][1]  # Get this value from the ar() operation above #coefficient correlation btw successive pts
fft_theor <- (1 - lag1^2) / (1 - 2 * lag1 * cos(f*2*pi) + lag1^2) #eqn 16 w/in cos() should have / length(f) # No - f is already equal to k/N in their notation
fft_theor <- var(Dt2, na.rm=TRUE) * fft_theor / (2 * length(f)) #rescaling for unit variance?
chi2 = qchisq(0.05, 2, lower.tail=FALSE) / 2
signif = chi2*fft_theor


ggplot(DsD, aes(Period, Freq_sq))+
  geom_line()+
  geom_line(aes(Period, Signf, color= "red"))+
  facet_wrap(~Year)

```